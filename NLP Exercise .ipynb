{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello My name is Paritosh and I'm looking for a job in DataScience\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(nltk.unigrams(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n"
     ]
    }
   ],
   "source": [
    "print (ps.stem(\"jumping\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'complic'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorterStemmer().stem(\"complications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "len= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf\n"
     ]
    }
   ],
   "source": [
    "print (len.lemmatize('leaves'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Term Frequency: (No. of terms (t) Appears in a document ) / (Total no. of terms in the Documents )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IDF :- It means how important a term is"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A document containing 100 words & the word 'cat' appears 3 times: \n",
    "\n",
    "TF: = 3/100 = 0.03"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now Assume that we have 10 million documents & the word 'cat' appears in 1000 times \n",
    "\n",
    "IDF:- log (10,000,000 / 1000) = 4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NLP Pipeline:\n",
    "\n",
    "1. Text Gathering\n",
    "2. Text Cleaning (Stemming , Lemmetization)\n",
    "3. Feature Generation (Bag of Words)\n",
    "4. Embedding and Senetence Representation (Word2Vec)\n",
    "5. Training the model by leveraging neural nets or regression techniques\n",
    "6. Model evaluation\n",
    "7. Making adjustments to the model\n",
    "8. Deployment of the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Named Entity Recognition(NER)?\n",
    "\n",
    "Named entity recognition is a method to divide a sentence into categories."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ex: Neil Armstong of the US had landed on the  moon in 1969 will be categorized as\n",
    "\n",
    "Neil Armstong- name;The US – country;1969 – time(temporal token).\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The idea behind NER is to enable the machine to pull out entities like people, places, things, locations, monetary figures, and more."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "How is feature extraction done in NLP\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q. Name some popular models other than Bag of words?\n",
    "Ans: Latent semantic indexing, word2vec."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q. Explain briefly about word2vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q. What is Latent Semantic Indexing?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q. What are the metrics used to test an NLP model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What are some popular Python libraries used for NLP:\n",
    "\n",
    "Stanford’s CoreNLP, SpaCy , NLTK and TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program : program\n",
      "programs : program\n",
      "programer : program\n",
      "programing : program\n",
      "programers : program\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "  \n",
    "# choose some words to be stemmed \n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "\n",
    "for w in words:\n",
    "    print (w , \":\" , ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumping : jump\n",
      "leaves : leav\n",
      "changes : chang\n",
      "fasting : fast\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = [\"jumping\", \"leaves\" , \"changes\", \"fasting\"]\n",
    "\n",
    "for w in words:\n",
    "    print (w, \":\" , ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumping : jumping\n",
      "leaves : leaf\n",
      "changes : change\n",
      "fasting : fasting\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "words = [\"jumping\", \"leaves\" , \"changes\", \"fasting\"]\n",
    "\n",
    "\n",
    "for w in words:\n",
    "    print (w, \":\", lm.lemmatize(w))\n",
    "\n",
    "print (lm.lemmatize(\"better\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better : good\n",
      "jumping : jumping\n",
      "leaves : leaf\n",
      "changes : change\n",
      "fasting : fasting\n",
      "rocks : rock\n",
      "better : better\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "words = [\"jumping\", \"leaves\" , \"changes\", \"fasting\", \"rocks\", \"better\"]\n",
    "\n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) \n",
    "\n",
    " \n",
    "for w in words:\n",
    "    print (w, \":\", lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Normalization : \n",
    "\n",
    "eliminating the punctuation , converting the entire text into lowercase or uppercase, converting numbers into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He   Hmm, Why not \n"
     ]
    }
   ],
   "source": [
    "text = \"He ? Hmm, Why not!\"\n",
    "\n",
    "punct = \" ? !\"\n",
    "\n",
    "for string in punct:\n",
    "    \n",
    "    text = text.replace (string , \" \")\n",
    "    \n",
    "print (text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "POS Tagging : Why we Use?\n",
    "\n",
    "When a word is being used twice in a sentence and has two different meaning.\n",
    "        then we need to know which word is being used in order to pronounce the text correctly.\n",
    "        \n",
    "        \n",
    "Example : They refuse (it's verb (deny)) to permit us to obtain the refuse permit ( it's noun (trash))."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Types of POS:\n",
    "    \n",
    "    1. Rule_Based POS\n",
    "    2. Stochastic POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
